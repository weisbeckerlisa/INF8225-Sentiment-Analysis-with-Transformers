{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"YSPNSw5nkAD9","executionInfo":{"status":"ok","timestamp":1714370255725,"user_tz":240,"elapsed":5764,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["!pip install -q transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tGJh9ZDficEB"},"outputs":[],"source":["import tensorflow_datasets as tfds\n","from transformers import TFBertForSequenceClassification\n","import tensorflow as tf\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","import numpy as np\n","\n","import os\n","os.environ['TF_USE_LEGACY_KERAS'] = '1'"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"xKxmY771jJJ0","executionInfo":{"status":"ok","timestamp":1714370275191,"user_tz":240,"elapsed":254,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["## Hyperparamètres\n","\n","# Longueur maximal en entrée (512 au maximal pour Bert)\n","max_length = 512\n","#Taille d'échantillon\n","batch_size = 6\n","# Taux d'apprentissages possibles pour Adam 2e-5, 3e-5 et 5e-5\n","learning_rate = 2e-5\n","# nombre d'epoch, maximum 2 pour des raisons de temps d'execution et de calcul\n","number_of_epochs = 1\n","# Nombre de couches gelé (maximum 12, le nombre de couches dans Bert)\n","freeze_layers = 10\n","# Nombre couches supplémentaires\n","add_layers = 1"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"PWgciYU7jD_l","executionInfo":{"status":"ok","timestamp":1714370278982,"user_tz":240,"elapsed":849,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["def convert_example_to_feature(review):\n","  return tokenizer.encode_plus(review,\n","                add_special_tokens = True, # ajout des tokens [CLS], [SEP]\n","                max_length = max_length, # Longueur maximal du texte fournit à Bert\n","                pad_to_max_length = True, # ajout du token [PAD]\n","                return_attention_mask = True, # Ajout du mask pour eviter la concentration sur le token [PAD]\n","              )"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sBBRAzBejMf6","executionInfo":{"status":"ok","timestamp":1714370282527,"user_tz":240,"elapsed":6,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n","  return {\n","      \"input_ids\": input_ids,\n","      \"token_type_ids\": token_type_ids,\n","      \"attention_mask\": attention_masks,\n","  }, label\n","\n","def encode_examples(ds, limit=-1):\n","  \"\"\"Prends en entrée un ensemble de données et l'encode de sorte à pouvoir être utilisé par Bert\"\"\"\n","  input_ids_list = []\n","  token_type_ids_list = []\n","  attention_mask_list = []\n","  label_list = []\n","  if (limit > 0):\n","      ds = ds.take(limit)\n","  for review, label in tfds.as_numpy(ds):\n","    bert_input = convert_example_to_feature(review.decode())\n","    input_ids_list.append(bert_input['input_ids'])\n","    token_type_ids_list.append(bert_input['token_type_ids'])\n","    attention_mask_list.append(bert_input['attention_mask'])\n","    label_list.append([label])\n","  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hya0PQA1i2cM"},"outputs":[],"source":["# Recuperation des données \"imdb review\" pour l'ensemble de test et de validation\n","(ds_train, ds_validation), ds_info = tfds.load('imdb_reviews',\n","          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n","          as_supervised=True,\n","          with_info=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tT-t5kTCj3hv"},"outputs":[],"source":["# Encodage de l'ensemble d'entrainement\n","ds_train_encoded = encode_examples(ds_train).shuffle(10000).batch(batch_size)\n","\n","# Encodage de l'ensemble de validation\n","ds_validation_encoded = encode_examples(ds_validation).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jmc_6vZ5DlL_"},"outputs":[],"source":["# Recuperation des données pour l'ensemble de test 1 basé sur \"yelp polarity review\"\n","(ds_test_sentiment), ds_info = tfds.load('yelp_polarity_reviews',\n","          as_supervised=True,\n","          split = (tfds.Split.TEST),\n","          with_info=True)\n","\n","# Encodage de l'ensemble de test 1\n","ds_test_sentiment_encoded = encode_examples(ds_test_sentiment).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJuDJuzfLklt"},"outputs":[],"source":["# Recuperation des données pour l'ensemble de test 2 basé sur \"Glue\", attention, le split de validation est utilisé ici, celui de test de fonctionne pas et ne contient que des -1\n","ds_test_other = tfds.load(name=\"glue\", split=(tfds.Split.VALIDATION))\n","\n","def extract_sentence_label(example):\n","  \"\"\"Prend en entrée un ensemble de donnée et en extrait les textes et les étiquettes\"\"\"\n","  return example['sentence'], example['label']\n","\n","# Application de la fonction sur l'ensemble de test 2 pour le mettre au même format que les autres ensembles de données (suppression des indices des données)\n","ds_test_other = ds_test_other.map(extract_sentence_label)\n","\n","# Encodage de l'ensemble de test 2\n","ds_test_other_encoded = encode_examples(ds_test_other).batch(batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kxhahzTfw9q0"},"outputs":[],"source":["# Importation du modèle préentrainé\n","model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n","\n","# Gèle d'un nombre spécifique de couches\n","for i in range(freeze_layers):\n","  model.bert.encoder.layer[i].trainable = False"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cQYdMwxbxAmT","executionInfo":{"status":"ok","timestamp":1714371786987,"user_tz":240,"elapsed":15616,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["## Ajout de nouvelles couches à la suite du modèle\n","if add_layers != 0:\n","\n","# Définition des entrées et du mask d'attention\n","  input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"input_ids\")\n","  attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name=\"attention_mask\")\n","\n","# Récupération de la sortie de la dernière couche du modèle préentrainé de Bert\n","  outputs = model([input_ids, attention_mask])[0]\n","\n","# Définition de la couche dense et du dropout de la première couche additionnelle\n","  dense_layer1 = tf.keras.layers.Dense(256, activation='relu')\n","  dropout_layer = tf.keras.layers.Dropout(0.1)\n","# Application de ces couches à la suite du modèle préentrainé\n","  x = dense_layer1(outputs)\n","  x = dropout_layer(x)\n","\n","\n","\n","# Définition et ajout de même d'autant de couches supplémentaires que demandé\n","  for i in range(add_layers - 1) :\n","    dense_layer = tf.keras.layers.Dense(256, activation='relu')\n","    dropout_layer = tf.keras.layers.Dropout(0.1)\n","    x = dense_layer(x)\n","    x = dropout_layer(x)\n","# Ajout d'une couche de sortie à deux classes\n","  dense_layer_final = tf.keras.layers.Dense(2, activation='softmax')\n","  x = dense_layer_final(x)\n","\n","# Création du modèle complet à partir des différentes couches\n","  model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=x)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Y_jd5yUjj74F","executionInfo":{"status":"ok","timestamp":1714371896205,"user_tz":240,"elapsed":3,"user":{"displayName":"Elie Bendelac","userId":"09307580510568081738"}}},"outputs":[],"source":["# Initialisation d'un optimiseur Adam\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n","\n","# Initialisation de la perte et de la prédiction, ici, nous n'avons pas de one-hot-vector\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","\n","#Compilation du modèle pour utilisation\n","model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvh6apJEusdB"},"outputs":[],"source":["# Entrainement du modèle et test de celui-ci sur l'ensemble d'évaluation\n","history_and_validation = model.fit(ds_train_encoded, epochs=number_of_epochs, validation_data=ds_validation_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6o16QO-GSZoW"},"outputs":[],"source":["# Test du modèle sur l'ensemble de test 1\n","results_1 = model.evaluate(ds_test_sentiment_encoded)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kK1bCkw5ijqr"},"outputs":[],"source":["# Test du modèle sur l'ensemble de test 2\n","results_2 = model.evaluate(ds_test_other_encoded)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}